{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53639689",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9156812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import modin.pandas as mpd\n",
    "import ray\n",
    "import ray.data\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607adde",
   "metadata": {},
   "source": [
    "# Reading the File with Different Approaches\n",
    "In this section, we will explore various methods for reading the file. Each approach has its own advantages and use cases depending on the file format, size, and the requirements of the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc819e",
   "metadata": {},
   "source": [
    "### 1.Reading the file with pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4455c3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas read time: 352.2310583591461 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the time to read the file using pandas\n",
    "start_time = time.time()\n",
    "df_pandas = pd.read_csv('Trips_merged.csv')\n",
    "end_time = time.time()\n",
    "print(f\"Pandas read time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf703c02",
   "metadata": {},
   "source": [
    "### 2.Reading the file with Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d290db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask read time: 0.38047361373901367 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the time to read the file using dask\n",
    "start_time = time.time()\n",
    "df_dask = dd.read_csv('Trips_merged.csv')\n",
    "#df_dask = df_dask.compute() # un comment it If your dataset is small enough to fit in memory.\n",
    "end_time = time.time()\n",
    "print(f\"Dask read time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07407a52",
   "metadata": {},
   "source": [
    "### 3.Reading the file with Ray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f4182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 22:24:02,967\tINFO worker.py:1781 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray read time: 3.579085111618042 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Measure the time to read the file using ray\n",
    "start_time = time.time()\n",
    "ds_ray = ray.data.read_csv('Trips_merged.csv')\n",
    "# df_ray = ds_ray.to_pandas()\n",
    "end_time = time.time()\n",
    "print(f\"Ray read time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac6009",
   "metadata": {},
   "source": [
    "### 4.Reading the file with Modin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06a2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 22:24:13,199\tINFO worker.py:1781 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modin read time: 363.9993145465851 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the time to read the file using modin\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "start_time = time.time()\n",
    "df_modin = mpd.read_csv('Trips_merged.csv')\n",
    "end_time = time.time()\n",
    "print(f\"Modin read time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50b61a",
   "metadata": {},
   "source": [
    "**Conclusion** : \n",
    "\n",
    "- **Dask** is the **fastest**, achieving a read time of approximately 0.38 seconds, indicating high efficiency for this operation.\n",
    "- Ray is also efficient but slightly slower than Dask, with a read time of about 3.58 seconds. It remains significantly faster than Pandas and Modin.\n",
    "- Pandas has a read time of approximately 352.23 seconds, making it considerably slower than both Dask and Ray.\n",
    "- **Modin** has the **slowest** read time at around 364.00 seconds.<br>\n",
    "\n",
    "**Note:** Performance can vary based on the nature of the data and specific configurations, so it's important to consider these factors when choosing a tool for data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758e192",
   "metadata": {},
   "source": [
    "# Creating Functions\n",
    "In this section, we will define custom functions to streamline our data processing and analysis tasks. Creating a Python file **(functions.py)** for user-defined functions so that we can use these functions whenever and wherever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c009627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "#for basic validation\n",
    "def clean_column_names(df):\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Replace any non-alphanumeric characters with underscores\n",
    "    df.columns= df.columns.str.replace('[^\\w]','_',regex=True)\n",
    "    \n",
    "    # remove any spaces\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#for validating columns \n",
    "def validate_columns(df, Yaml_file_name):\n",
    "    # checking if the length and file names are same\n",
    "    if list(df.columns) == Yaml_file_name['columns'] and len(df.columns) == len(Yaml_file_name['columns']):\n",
    "        print(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        if list(df.columns) != Yaml_file_name['columns']:\n",
    "            print(\"column names are different,please check column names in the uploaded file\",set((df.columns))-set(Yaml_file_name['columns']))\n",
    "        if len(df.columns) != len(Yaml_file_name['columns']):\n",
    "            print(\"columns length are differnt, please add or remove extra columns.\")                    \n",
    "        return 0\n",
    "    \n",
    "# for opening/reading Yaml File\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        try:\n",
    "            return yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be39854",
   "metadata": {},
   "source": [
    "# Basic Validation\n",
    "Instead of hard-coding the validation, we use custom **clean_column_names** function from the **functions.py** file. This function:\n",
    "\n",
    "- Convert all column names to lowercase.\n",
    "- Remove spaces.\n",
    "- Replace non-alphanumeric characters with underscores.<br>\n",
    "\n",
    "**Note**: You can add more validations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51fff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the file which we created.\n",
    "import functions as fn\n",
    "\n",
    "# Performing basic validation using the function \"clean_column_names\" \n",
    "df_pandas=fn.clean_column_names(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616e8df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>from_station_id</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>to_station_id</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>user_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4118</td>\n",
       "      <td>2013-06-27T17:11:00Z</td>\n",
       "      <td>2013-06-27T17:16:00Z</td>\n",
       "      <td>480.0</td>\n",
       "      <td>300</td>\n",
       "      <td>85</td>\n",
       "      <td>Michigan Ave &amp; Oak St</td>\n",
       "      <td>28</td>\n",
       "      <td>Larrabee St &amp; Menomonee St</td>\n",
       "      <td>Casual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4275</td>\n",
       "      <td>2013-06-27T19:44:00Z</td>\n",
       "      <td>2013-06-27T19:45:00Z</td>\n",
       "      <td>77.0</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>Racine Ave &amp; Congress Pkwy</td>\n",
       "      <td>32</td>\n",
       "      <td>Racine Ave &amp; Congress Pkwy</td>\n",
       "      <td>Casual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4291</td>\n",
       "      <td>2013-06-27T19:58:00Z</td>\n",
       "      <td>2013-06-27T20:05:00Z</td>\n",
       "      <td>77.0</td>\n",
       "      <td>420</td>\n",
       "      <td>32</td>\n",
       "      <td>Racine Ave &amp; Congress Pkwy</td>\n",
       "      <td>19</td>\n",
       "      <td>Loomis St &amp; Taylor St</td>\n",
       "      <td>Casual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4316</td>\n",
       "      <td>2013-06-27T20:06:00Z</td>\n",
       "      <td>2013-06-27T20:09:00Z</td>\n",
       "      <td>77.0</td>\n",
       "      <td>180</td>\n",
       "      <td>19</td>\n",
       "      <td>Loomis St &amp; Taylor St</td>\n",
       "      <td>19</td>\n",
       "      <td>Loomis St &amp; Taylor St</td>\n",
       "      <td>Casual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4342</td>\n",
       "      <td>2013-06-27T20:13:00Z</td>\n",
       "      <td>2013-06-27T20:27:00Z</td>\n",
       "      <td>77.0</td>\n",
       "      <td>840</td>\n",
       "      <td>19</td>\n",
       "      <td>Loomis St &amp; Taylor St</td>\n",
       "      <td>55</td>\n",
       "      <td>Halsted St &amp; James M Rochford St</td>\n",
       "      <td>Casual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_id            start_time             stop_time  bike_id  trip_duration  \\\n",
       "0    4118  2013-06-27T17:11:00Z  2013-06-27T17:16:00Z    480.0            300   \n",
       "1    4275  2013-06-27T19:44:00Z  2013-06-27T19:45:00Z     77.0             60   \n",
       "2    4291  2013-06-27T19:58:00Z  2013-06-27T20:05:00Z     77.0            420   \n",
       "3    4316  2013-06-27T20:06:00Z  2013-06-27T20:09:00Z     77.0            180   \n",
       "4    4342  2013-06-27T20:13:00Z  2013-06-27T20:27:00Z     77.0            840   \n",
       "\n",
       "  from_station_id           from_station_name to_station_id  \\\n",
       "0              85       Michigan Ave & Oak St            28   \n",
       "1              32  Racine Ave & Congress Pkwy            32   \n",
       "2              32  Racine Ave & Congress Pkwy            19   \n",
       "3              19       Loomis St & Taylor St            19   \n",
       "4              19       Loomis St & Taylor St            55   \n",
       "\n",
       "                    to_station_name user_type gender  birth_year  \n",
       "0        Larrabee St & Menomonee St    Casual    NaN         NaN  \n",
       "1        Racine Ave & Congress Pkwy    Casual    NaN         NaN  \n",
       "2             Loomis St & Taylor St    Casual    NaN         NaN  \n",
       "3             Loomis St & Taylor St    Casual    NaN         NaN  \n",
       "4  Halsted St & James M Rochford St    Casual    NaN         NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37582c",
   "metadata": {},
   "source": [
    "# Creating a YAML File\n",
    "In this section, we will create a YAML file to define the structure and requirements of our dataset. This file will specify the expected columns, their names, and any additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4864ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Trips.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile Trips.yaml\n",
    "file_type: csv\n",
    "dataset_name: Cyclistic_trips\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "columns: \n",
    "    - trip_id\n",
    "    - start_time\n",
    "    - stop_time\n",
    "    - bike_id\n",
    "    - trip_duration\n",
    "    - from_station_id\n",
    "    - from_station_name\n",
    "    - to_station_id\n",
    "    - to_station_name\n",
    "    - user_type\n",
    "    - gender\n",
    "    - birth_year   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b44b0",
   "metadata": {},
   "source": [
    "# Validation (YAML File)\n",
    "#### Column Validation Using Custom Function\n",
    "In this section, we will use the custom **validate_columns function** to ensure that the columns in our dataset match the specifications defined in a YAML file. The function will check both the number of columns and their names.\n",
    "\n",
    "- If the validation is successful, a success message will be printed.\n",
    "- If the validation fails, an error message will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177118a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_type': 'csv',\n",
       " 'dataset_name': 'Cyclistic_trips',\n",
       " 'inbound_delimiter': ',',\n",
       " 'outbound_delimiter': '|',\n",
       " 'columns': ['trip_id',\n",
       "  'start_time',\n",
       "  'stop_time',\n",
       "  'bike_id',\n",
       "  'trip_duration',\n",
       "  'from_station_id',\n",
       "  'from_station_name',\n",
       "  'to_station_id',\n",
       "  'to_station_name',\n",
       "  'user_type',\n",
       "  'gender',\n",
       "  'birth_year']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the YAML file \n",
    "Trips_YAML_file = fn.read_config_file(\"Trips.yaml\")\n",
    "Trips_YAML_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f68c020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name and column length validation passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validating the columns using custom function\n",
    "fn.validate_columns(df_pandas,Trips_YAML_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef51cf6",
   "metadata": {},
   "source": [
    "#### Example: Testing Failed Validation\n",
    "To verify that our validation function correctly identifies errors, weâ€™ll create a dataset with intentional mismatches. This example will help us ensure that the validation function properly detects and reports failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0036ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'start_time': ['2024-01-01 08:00', '2024-01-01 09:00'],   # Correct column\n",
    "    'stop_time': ['2024-01-01 08:30', '2024-01-01 09:30'],    # Correct column\n",
    "    'bike_id': ['B123', 'B124'],                             # Correct column\n",
    "    'duration': [30, 30],                                    # Incorrect column name (should be 'trip_duration')\n",
    "    'from_station_name': ['Station A', 'Station B'],         # Correct column\n",
    "    'to_station_id': [100, 101],                             # Correct column\n",
    "    'user_type': ['Subscriber', 'Customer'],                 # Correct column\n",
    "    'birth_year': [1985, 1990],                              # Correct column\n",
    "    'extra_column': ['extra', 'data']                        # Extra column not in expected list\n",
    "}\n",
    "\n",
    "# Creating DataFrame\n",
    "df_Example = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a735a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names are different,please check column names in the uploaded file {'duration', 'extra_column'}\n",
      "columns length are differnt, please add or remove extra columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example function call\n",
    "fn.validate_columns(df_Example, Trips_YAML_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b544a3",
   "metadata": {},
   "source": [
    "# Converting CSV File to Pipe-Separated Text File (|) and Compressing to GZ Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37ea957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.to_csv(\"output_file_gz_format.csv.gz\",\n",
    "          index=False,\n",
    "          sep = (\"|\"),\n",
    "          compression=\"gzip\",\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dc2cd",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This section provides a summary of the dataset, including the total number of rows and columns, as well as the file sizes before and after compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afec4db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Total rows: ==>   33,356,927\n",
      "Total columns: ==>   12\n",
      "Original File size: ==>   4,430,497,816 bytes (4.13 GB)\n",
      "File size of pipe-separated text file (|) in gz format: ==>   856,806,404 bytes (0.80 GB)\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(f\"Total rows: {'==>':<5} {len(df_pandas):,}\")\n",
    "print(f\"Total columns: {'==>':<5} {len(df_pandas.columns):,}\")\n",
    "print(f\"Original File size: {'==>':<5} {os.path.getsize('Trips_merged.csv'):,} bytes ({os.path.getsize('Trips_merged.csv')/(1024**3):.2f} GB)\")\n",
    "print(f\"File size of pipe-separated text file (|) in gz format: {'==>':<5} {os.path.getsize('output_file_gz_format.csv.gz'):,} bytes ({os.path.getsize('output_file_gz_format.csv.gz')/(1024**3):.2f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef3a3c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd34ce",
   "metadata": {},
   "source": [
    "I found the dataset(4GB) on Kaggle's site [Cyclistic dataset (clean & merge)](https://www.kaggle.com/datasets/algorismus/cyclistic-dataset-clean-merge/data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
